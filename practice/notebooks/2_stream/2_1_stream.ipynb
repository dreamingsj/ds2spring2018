{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download additional dependencies for Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm -rf /home/ubuntu/spark-streaming-*.jar*\n",
    "wget http://central.maven.org/maven2/org/apache/spark/spark-streaming-kafka-0-8-assembly_2.11/2.3.0/spark-streaming-kafka-0-8-assembly_2.11-2.3.0.jar -P /home/ubuntu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Spark standalone cluster \n",
    "* (instead of YARN for this practice, to easily see the Spark Web UI)\n",
    "* Check the cluster UI => MASTER_PUBLIC_IP:8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "/home/ubuntu/spark_scripts/stop_cluster.sh\n",
    "/home/ubuntu/spark_scripts/start_cluster.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Running a simple wordcount query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement a simple continuous wordcount query. This query will\n",
    "* Read the text sentence from a Kafka\n",
    "* Split the sentence into words\n",
    "* Continuously aggregate the counts for each word\n",
    "\n",
    "Firstly, we need to start from making a simple TCP server on the master server which produces random sentences to its clients. In this class, we will use `nc (netcat)` program. Open a terminal from your notebook and type `nc -lk 20332`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "import json\n",
    "\n",
    "findspark.init('/home/ubuntu/spark')\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "# Create a spark context\n",
    "conf = SparkConf().setAppName(\"WordCount\").setMaster(\"spark://master:7077\").set(\"spark.jars\", \"/home/ubuntu/spark-streaming-kafka-0-8-assembly_2.11-2.3.0.jar\")\n",
    "spark_context = SparkContext(conf=conf)\n",
    "# Create a streaming context with batch interval 5 secs\n",
    "stream_context = StreamingContext(spark_context, 5)\n",
    "# Read the text line from the socket stream\n",
    "lines = stream_context.socketTextStream(\"master\", 20332)\n",
    "# Split each line into multiple words\n",
    "words = lines.flatMap(lambda x: x.split(\" \"))\n",
    "# Make a (key(word), count) stream and Count the word\n",
    "word_counts = words.map(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "    \n",
    "word_counts.pprint()\n",
    "\n",
    "stream_context.start()\n",
    "# Wait for 60 seconds\n",
    "stream_context.awaitTermination(60)\n",
    "stream_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_context.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Running a stream application from Kafka source\n",
    "\n",
    "Apache Kafka is a distributed streaming platform which supports messaging, processing, and storing of the stream data. In this practice session, we will focus on leveraging Kafka as a message brokering system.\n",
    "\n",
    "Kafka supports high-throughput & fault-tolerant messaging via publish-subscribe model. In publish-subscribe model, stream events are managed in **topics**. A **Producer** consistently generates a data, whereas **Consumer** receives the data events. Each topic is partitioned into multiple \"partitions\", and partitions are distributed and stored in the secondary storage to guarantee fault tolerance.\n",
    "\n",
    "As we can guess from the information above, we need the server address and topic name to fetch the data from a Kafka broker. Kafka server and producers are already set up by TAs. We will review the Producer code firstly.\n",
    "\n",
    "After revewing the code, we will implement the same word count application from the Kafka source. The broker address is **147.46.216.122:9092** and the topic is **wc**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "import json\n",
    "\n",
    "findspark.init('/home/ubuntu/spark')\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "# Create a spark context containing the additional Kafka dependency.\n",
    "conf = SparkConf().setAppName(\"KafkaWordCount\").setMaster(\"spark://master:7077\").set(\"spark.jars\", \"/home/ubuntu/spark-streaming-kafka-0-8-assembly_2.11-2.3.0.jar\")\n",
    "spark_context = SparkContext(conf=conf)\n",
    "# Create a streaming context with batch interval 5 secs\n",
    "stream_context = StreamingContext(spark_context, 5)\n",
    "# Read the Kafka data stream from the broker for the given topic.\n",
    "# The data will be arrived in (topic, data) format\n",
    "kvs = KafkaUtils.createDirectStream(stream_context, [\"wc\"], {\"metadata.broker.list\": \"147.46.216.122:9092\"})\n",
    "# Extract the data from the Kafka data stream\n",
    "lines = kvs.map(lambda x: x[1]).map(lambda l: l.replace('\"', ''))\n",
    "# Split each line into multiple words\n",
    "words = lines.flatMap(lambda x: x.split(\" \"))\n",
    "# Count the word\n",
    "word_counts = words.map(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "word_counts.pprint()\n",
    "\n",
    "# Start the computation\n",
    "stream_context.start()\n",
    "# await termination for 60 seconds\n",
    "stream_context.awaitTermination(60)\n",
    "stream_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_context.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, we processed only the simple plain texts. From this time, we will process JSON-formatted data events which are widely used for data transfer. Here, we will get the json-formatted movie datasets from the kafka server. We can get the data from the **movie** topic.\n",
    "\n",
    "To  json-formatted data, we will use python json `json` package. You can get the python dictionary instance by calling `json.loads` on serialized json data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "import json\n",
    "\n",
    "findspark.init('/home/ubuntu/spark')\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "# Create a spark context\n",
    "conf = SparkConf().setAppName(\"movie_json\").setMaster(\"spark://master:7077\").set(\"spark.jars\", \"/home/ubuntu/spark-streaming-kafka-0-8-assembly_2.11-2.3.0.jar\")\n",
    "spark_context = SparkContext(conf=conf)\n",
    "# Create a streaming context with batch interval 5 secs\n",
    "stream_context = StreamingContext(spark_context, 5)\n",
    "#lines = stream_context.socketTextStream(\"master\", 5000)\n",
    "kvs = KafkaUtils.createDirectStream(stream_context, [\"movie\"], {\"metadata.broker.list\": \"147.46.216.122:9092\"})\n",
    "# Deserialize the json-formattted data into python dict.\n",
    "movies = kvs.map(lambda x: json.loads(x[1]))\n",
    "# Print the input data\n",
    "movies.pprint()\n",
    "\n",
    "# Start the computation\n",
    "stream_context.start()\n",
    "# await termination for 60 seconds\n",
    "stream_context.awaitTermination(60)\n",
    "stream_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_context.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also perform some filtering on json data. For example, you can filter out the movies prodcued before 2000 from the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "import json\n",
    "\n",
    "findspark.init('/home/ubuntu/spark')\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "# Create a spark context\n",
    "conf = SparkConf().setAppName(\"filter_movie\").setMaster(\"spark://master:7077\").set(\"spark.jars\", \"/home/ubuntu/spark-streaming-kafka-0-8-assembly_2.11-2.3.0.jar\")\n",
    "spark_context = SparkContext(conf=conf)\n",
    "# Create a streaming context with batch interval 5 secs\n",
    "stream_context = StreamingContext(spark_context, 5)\n",
    "\n",
    "kvs = KafkaUtils.createDirectStream(stream_context, [\"movie\"], {\"metadata.broker.list\": \"147.46.216.122:9092\"})\n",
    "# Deserialize the json-formattted data into python dict.\n",
    "movies = kvs.map(lambda x: json.loads(x[1]))\n",
    "# Filter out movies produced before 2000.\n",
    "twentyfirstcentry_movies = movies.filter(lambda movie: movie['year'] >= 2000)\n",
    "# Print the input data\n",
    "twentyfirstcentry_movies.pprint()\n",
    "\n",
    "# Start the computation\n",
    "stream_context.start()\n",
    "# await termination for 60 seconds\n",
    "stream_context.awaitTermination(60)\n",
    "stream_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_context.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz 0: Filter & Map\n",
    "\n",
    "In the following cell, implement the word count example which gets the data from the **\"wc\"** topic according to the following condition\n",
    "* Filters out the word 'DS2'\n",
    "* Double the frequency of the word 'class'\n",
    "\n",
    "Hint: Make a separate method rather than lambda for filtering for easy coding.\n",
    "```\n",
    "def method(word):\n",
    "   ...\n",
    "   \n",
    "stream.map(method)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "import json\n",
    "\n",
    "findspark.init('/home/ubuntu/spark')\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "# Create a spark context\n",
    "conf = SparkConf().setAppName(\"quiz_0\").setMaster(\"spark://master:7077\").set(\"spark.jars\", \"/home/ubuntu/spark-streaming-kafka-0-8-assembly_2.11-2.3.0.jar\")\n",
    "spark_context = SparkContext(conf=conf)\n",
    "# Create a streaming context with batch interval 5 secs\n",
    "stream_context = StreamingContext(spark_context, 5)\n",
    "kvs = KafkaUtils.createDirectStream(stream_context, [\"wc\"], {\"metadata.broker.list\": \"147.46.216.122:9092\"})\n",
    "\n",
    "# TODO: Implement your codes here!\n",
    "\n",
    "# Start the computation\n",
    "stream_context.start()\n",
    "# await termination for 60 seconds\n",
    "stream_context.awaitTermination(60)\n",
    "stream_context.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz 1: Filtering on JSON-formatted data\n",
    "\n",
    "In the following cell, implement the stream application which receives the **movie** topic from the Kafka stream and filters out all the movies which does not contain the word \"the\" in their (cases are ignored). Print out the titles of the movies which are not filtered out.\n",
    "\n",
    "## Example\n",
    "\n",
    "**Input**: {\"title\": \"The titanic\", ...}, {\"title\": \"Titanic\", ...}, {\"title\": \"Flintheart Glomgold\", ...}, ...\n",
    "\n",
    "**Output**: \"The titanic\", \"Flintheart Glombold\", ...\n",
    "\n",
    "Hint: Use `.lower()` method and `in` operator.\n",
    "Ex) \n",
    "```\n",
    ">>> a = \"Hello\" \n",
    ">>> a.lower() \n",
    "\"hello\"\n",
    ">>> \"llo\" in a\n",
    "True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "import json\n",
    "\n",
    "findspark.init('/home/ubuntu/spark')\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "# Create a spark context\n",
    "conf = SparkConf().setAppName(\"quiz_1\").setMaster(\"spark://master:7077\").set(\"spark.jars\", \"/home/ubuntu/spark-streaming-kafka-0-8-assembly_2.11-2.3.0.jar\")\n",
    "spark_context = SparkContext(conf=conf)\n",
    "# Create a streaming context with batch interval 5 secs\n",
    "stream_context = StreamingContext(spark_context, 5)\n",
    "# Get the data from the Kafka Stream\n",
    "kvs = KafkaUtils.createDirectStream(stream_context, [\"movie\"], {\"metadata.broker.list\": \"147.46.216.122:9092\"})\n",
    "\n",
    "# TODO: Implement your code here!\n",
    "\n",
    "# Start the computation\n",
    "stream_context.start()\n",
    "# await termination for 60 seconds\n",
    "stream_context.awaitTermination(60)\n",
    "stream_context.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Running a windowed stream application\n",
    "\n",
    "By windowing, we can continuously get the set of recent data. A time-based **sliding window** can be defined by **window size** and **sliding interval**. For example, the window of `(window size = 5 seconds, sliding interval = 1 seconds)` consistently emits the data events in recent five seconds for every one second. For the special cases when the window size and the sliding interval is same, we call them as **tumbling windows**.\n",
    "\n",
    "Let's make a windowed movie stream, with window size of 30 second and sliding interval of 5 second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "import json\n",
    "\n",
    "findspark.init('/home/ubuntu/spark')\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "# Create a spark context\n",
    "conf = SparkConf().setAppName(\"window\").setMaster(\"spark://master:7077\").set(\"spark.jars\", \"/home/ubuntu/spark-streaming-kafka-0-8-assembly_2.11-2.3.0.jar\")\n",
    "spark_context = SparkContext(conf=conf)\n",
    "# Create a streaming context with batch interval 5 secs\n",
    "stream_context = StreamingContext(spark_context, 5)\n",
    "\n",
    "kvs = KafkaUtils.createDirectStream(stream_context, [\"movie\"], {\"metadata.broker.list\": \"147.46.216.122:9092\"})\n",
    "# Deserialize the json-formattted data into python dict.\n",
    "movies = kvs.map(lambda x: json.loads(x[1]))\n",
    "# Make a time-based window (size = 30 secs, interval = 5 secs)\n",
    "windowed_movies = movies.window(30, 5)\n",
    "# Print the window\n",
    "windowed_movies.pprint()\n",
    "\n",
    "# Start the computation\n",
    "stream_context.start()\n",
    "# await termination for 60 seconds\n",
    "stream_context.awaitTermination(60)\n",
    "stream_context.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quiz 2: Windowed aggregation\n",
    "\n",
    "Windowed streams can also be aggregated like normal stream data. Make the windowed stream of movie data firstly (size: 30 secs, interval: 5 secs). On the windowed stream, make a python `dict` whose key is the first alphabet of the movie title and the value is the list of movie titles starting with the alphabet. Before aggregation, make sure that all the movie titles are lower-cased.\n",
    "\n",
    "## Example\n",
    "\n",
    "Input data in window: {\"title\": \"titanic\", ...}, {\"title\": \"Harry Porter\", ...}, {\"title\": \"The Purchase Price\"}\n",
    "\n",
    "Output: {\"h\": \\[\"harry porter\"\\], \"t\": \\[\"titanic\", \"the purchace price\"\\]}\n",
    "\n",
    "Hint: Make a keyed stream firstly before making a windowed stream. Also, use list appending in Python.\n",
    "```\n",
    ">>> a = [1, 2, 3]\n",
    ">>> b = [4, 5]\n",
    ">>> a + b\n",
    "[1, 2, 3, 4, 5]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "import json\n",
    "\n",
    "findspark.init('/home/ubuntu/spark')\n",
    "\n",
    "from pyspark import SparkConf\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "# Create a spark context\n",
    "conf = SparkConf().setAppName(\"quiz_2\").setMaster(\"spark://master:7077\").set(\"spark.jars\", \"/home/ubuntu/spark-streaming-kafka-0-8-assembly_2.11-2.3.0.jar\")\n",
    "spark_context = SparkContext(conf=conf)\n",
    "# Create a streaming context with batch interval 5 secs\n",
    "stream_context = StreamingContext(spark_context, 5)\n",
    "\n",
    "# TODO: Implement your code here!\n",
    "\n",
    "# Start the computation\n",
    "stream_context.start()\n",
    "# await termination for 60 seconds\n",
    "stream_context.awaitTermination(60)\n",
    "stream_context.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Running a structured stream application\n",
    "\n",
    "Spark **structured stream** offers high-level stream processing using Spark's **Dataframe** API. Using structured stream, we can apply SQL-like operations on continuously incoming datastreams.\n",
    "Due to the limitation of jupyter notebook, we will perform the practice session on linux terminal. Before we start this session, prepare a new Jupyter terminal which would be used for executing python scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/home/ubuntu/ds2spring2018’: File exists\n",
      "fatal: destination path '/home/ubuntu/ds2spring2018' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Clone & setup the example codes\n",
    "mkdir /home/ubuntu/ds2spring2018\n",
    "git clone https://github.com/swsnu/ds2spring2018.git /home/ubuntu/ds2spring2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the running example for the TCP-word count using structured stream.\n",
    "\n",
    "You can execute this code by the following command\n",
    "`/home/ubuntu/spark/bin/spark-submit /home/ubuntu/ds2spring2018/practice/structured_wc.py`\n",
    "\n",
    "To see the result, open another terminal from your notebook, run `nc -lk 20332`, and type random sentences.\n",
    "\n",
    "**NOTE: Do not run your code on Jupyter notebook. Run it on your terminal instead**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "findspark.init('/home/ubuntu/spark')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "## Make a spark sql session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StructuredSocketWordCount\") \\\n",
    "    .master('spark://master:7077') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "## Get the spark data from TCP socket\n",
    "lines = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"master\") \\\n",
    "    .option(\"port\", 20332) \\\n",
    "    .load()\n",
    "\n",
    "## Make the new column \"word\" by splitting the line\n",
    "words = lines.select(\n",
    "    explode(\n",
    "        split(lines.value, \" \")\n",
    "    ).alias(\"word\")\n",
    ")\n",
    "\n",
    "wordCounts = words.groupBy(\"word\").count()\n",
    "\n",
    "query = wordCounts \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Using `structured stream`, you can also easily consume JSON-formatted data from Kafka stream, and perform relational operations (selection, projection, ...) on them. Here is a sample code which selects the `title` of the movies whose `year` is less than 2000. To run this code, you need to add additional kafka dependency. The running script would be `/home/ubuntu/bin/spark-submit run --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.0 /home/ubuntu/ds2stream2018/practice/structured_json.py`.\n",
    "\n",
    "**NOTE: Do not run your code on Jupyter notebook. Run it on your terminal instead**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init('/home/ubuntu/spark')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import col, from_json, get_json_object\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Make a spark sql session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StructuredSocketWordCount\") \\\n",
    "    .master('spark://master:7077') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Get the json-formatted data from Kafka stream\n",
    "kafka_movies = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"147.46.216.122:9092\") \\\n",
    "    .option(\"subscribe\", \"movie\") \\\n",
    "    .load()\n",
    "\n",
    "# Define the relational schema\n",
    "schema = StructType().add(\"title\", StringType()).add(\"genre\", StringType()).add(\"year\", LongType())\n",
    "\n",
    "# Change the JSON events into relational tuples\n",
    "relational_movies = kafka_movies.select([get_json_object(col(\"value\").cast(\"string\"), \"$.{}\".format(c)).alias(c)\n",
    "    for c in [\"title\", \"genre\", \"year\"]])\n",
    "\n",
    "# Change the type of year from string to integer\n",
    "relational_movies = relational_movies.select(col(\"title\"), col(\"genre\"), relational_movies.year.cast('integer').alias('year'))\n",
    "# Select the movie titles with year < 2000\n",
    "results = relational_movies.select(\"title\").where(\"year < 2000\")\n",
    "\n",
    "query = results \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination(60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
