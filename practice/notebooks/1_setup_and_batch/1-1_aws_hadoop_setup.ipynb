{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0AHbSfm9FwN3"
   },
   "source": [
    "# 0. Jupyter 101\n",
    "\n",
    "Let's look around Jupyter environments.\n",
    "* **What is cell?** There are (more than) two type of cells: `Markdown` and `code`. You will get what they mean intuitively.\n",
    "* **How to run a cell?** Activate a cell below and run it by pressing `Ctrl (or Cmd) + Return` or `Shift + Return`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup Network Configurations\n",
    "\n",
    "* To use hadoop (and most SWs in the hadoop ecosystem), we need configure\n",
    "    * Hosts and hostname to let all machines be accessible\n",
    "    * SSH keys since most programs are executed in remote machines through SSH\n",
    "    * Profiles such as paths, environment variables\n",
    "* You will modify `/etc/hosts`, `/etc/hostname`, `~/.ssh/authorized_keys`, `/etc/environment`\n",
    "\n",
    "\n",
    "## 1) /etc/hosts\n",
    "\n",
    "Instead of pointing each machine with an IP address, using hostname is much more handy (e.g., master, worker1, worker2). We need to modify `/etc/hosts` to register the IP addresses and hostnames.\n",
    "\n",
    "**[TODO]** Replace *[MASTER|WORKER]_PRIVATE_IP* with the *Private IPs* that you can see in the instances page.\n",
    " You may use public IPs but be aware that you need to update whenever you restart VMs since AWS changes the public IPs assigned to VMs when restarting them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "R3o5R_XeFvbN"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sudo cp ~/hosts.bak /etc/hosts # Just in case if you run more than once.\n",
    "echo '$MASTER_IP master' | sudo tee -a /etc/hosts\n",
    "echo '$WORKER1_IP worker1' | sudo tee -a /etc/hosts\n",
    "echo '$WORKER2_IP worker2' | sudo tee -a /etc/hosts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure all entries (master, worker1, worker2) are set properly.\n",
    "\n",
    "**[Note]** In the SSH commands below, \n",
    "* `-o` option allows us to login without typing `yes` for the option (it does not work in jupyter), and \n",
    "* `-i $KEY_FILE` option uses the given `pem` file for credential. \n",
    "\n",
    "The file is created when I issued the initial instance for creating AMI. We will temporarily use this key file until we add SSH credential below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "for host in master worker1 worker2\n",
    "do\n",
    "    # The below commands send SSH request to echo a string.\n",
    "    ssh -o \"StrictHostKeyChecking no\" -i ~/ssds2-2018.pem $host \"echo 'hi from $host'\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow other machines can identify other machines (at least master) as well, let's replicate `/etc/hosts` files to others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for host in worker1 worker2\n",
    "do\n",
    "    cat /etc/hosts | ssh -o \"StrictHostKeyChecking no\" -i ~/ssds2-2018.pem $host 'sudo tee /etc/hosts'\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) /etc/hostname\n",
    "\n",
    "By default, AWS sets a VM's hostname with its private IP (e.g., `ip-172-31-22-59`), which we don't want to use. Instead, let's change it to the ones that we (and Hadoop precisely) will use (e.g., master, worker1, worker2). You can set it by running `hostname $HOST_NAME`, but AWS will revert when the machine is rebooted so that you need to update the change again similar to private IP. The best solution is to modify `/etc/hostname` instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "4vN785YOF5_u"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "for host in master worker1 worker2\n",
    "do\n",
    "    ssh -o \"StrictHostKeyChecking no\" -i ~/ssds2-2018.pem $host \"sudo hostname $host\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply those changes, we need to reboot machines. Instead, let's change the hostname of the current active session as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for host in master worker1 worker2\n",
    "do\n",
    "    ssh -o \"StrictHostKeyChecking no\" -i ~/ssds2-2018.pem $host \"echo '$host' | sudo tee /etc/hostname\"\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) SSH setup for passwordless login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Hadoop, the machines should be able to login without password. Let's create an SSH key file and register the key file in the `authorized_keys`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Rxnc_xzNF-XY"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Create SSH keys on master\n",
    "for host in master worker1 worker2\n",
    "do\n",
    "    ssh -o \"StrictHostKeyChecking no\" -i ~/ssds2-2018.pem $host \"ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa\"\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for host in master worker1 worker2\n",
    "do\n",
    "    ssh -o \"StrictHostKeyChecking no\" -i ~/ssds2-2018.pem $host cp ~/authorized_keys.bak ~/.ssh/authorized_keys\n",
    "    ssh -o \"StrictHostKeyChecking no\" -i ~/ssds2-2018.pem $host cp ~/known_hosts.bak ~/.ssh/known_hosts\n",
    "\n",
    "    for host_to_scan in master worker1 worker2\n",
    "    do\n",
    "        ssh -o \"StrictHostKeyChecking no\" -i ~/ssds2-2018.pem $host_to_scan cat ~/.ssh/id_rsa.pub | ssh -o \"StrictHostKeyChecking no\" -i ~/ssds2-2018.pem $host \"tee -a ~/.ssh/authorized_keys\"        \n",
    "        ssh -o \"StrictHostKeyChecking no\" -i ~/ssds2-2018.pem $host \"ssh-keyscan -t rsa $host_to_scan >> ~/.ssh/known_hosts\"\n",
    "    done\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_cU6auk7GAiM"
   },
   "source": [
    "# 2. Setup Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Download and extract the hadoop binaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We downloaded the hadoop binary (from http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.0.1/hadoop-3.0.1.tar.gz) and extracted to home directory (`/home/ubuntu/hadoop`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "_zV2u4_6GBrG"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls /home/ubuntu/hadoop # Equivalently ~/hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Setup environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop processes (and related programs like Spark) identifies Hadoop's directory by looking up `HADOOP_HOME` environment variable - thus we need to set in the host environment. \n",
    "Besides, in order to make Hadoop executable in any location, the path should be added the `PATH` environment variable.\n",
    "To reduce the overhead, I modified `/etc/environment`. Let's see how it was \n",
    "Check whether the hadoop is added correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat /etc/environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Note]** You may ask why the variables are not set in the user-level profile (e.g., .profile, .bash_profile). This is because the variables are accessed via SSH and they were not found in many cases. Although you may not be able to change the system-wide configuration files (e.g., `/etc/environment`), I assumed that you have admin previledge to install frameworks like hadoop or spark on your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all set, `hadoop version` should work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hadoop version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Setup Hadoop configuration files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop's configuration is set via xml files in the `$HADOOP_HOME/etc/hadoop/etc`.\n",
    "\n",
    "To save time, we prepared the configuration files in your home directory.\n",
    "Just take a look at how the files look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### core-site.xml\n",
    "\n",
    "`core-site.xml` is for specifying the high-level configuration for the entire cluster such as file systems, security, high availability, etc. Most importantly, this configuration file consists of the location of the HDFS's namenode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat ~/core-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hdfs-site.xml\n",
    "`hdfs-site.xml` specifies the HDFS-specific configurations. For example, we can configure where we store the actual file blocks (in the Operating Systems's view), and replication, etc. Here we specify the location of the files and HDFS web UI's address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat ~/hdfs-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### workers\n",
    "`workers` (previously `slaves`) specifies which nodes will run as workers (i.e., DataNode in HDFS and NodeManager in YARN). You can just list up the hostnames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat ~/workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "DRewgYCWGDLm"
   },
   "outputs": [],
   "source": [
    "%%bash \n",
    "\n",
    "for host in master worker1 worker2\n",
    "do\n",
    "    ssh $host cp ~/core-site.xml ~/hadoop/etc/hadoop/core-site.xml\n",
    "    ssh $host cp ~/hdfs-site.xml ~/hadoop/etc/hadoop/hdfs-site.xml\n",
    "    ssh $host cp ~/workers ~/hadoop/etc/hadoop/workers\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Format Namenode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to format Namenode to initialize the metadata. Only thing you need to do is put the one-line command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs namenode -format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Start HDFS daemons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RyLpM0pfGEyX"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "start-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Explore HDFS Web UI!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to `http://<MASTER_IP>:50070`, and enjoy a nice web UI provided by HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ATPXOoGGGHzg"
   },
   "source": [
    "# 3. Use HDFS Commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Which commands you can use in HDFS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many commands that HDFS provides (Remember the commands that you used above `hdfs namenode format`).\n",
    "You will mostly use `hdfs dfs` command, which is for the file system interface (similar to `ls`, `cp`, `mv`, `rm`).\n",
    "\n",
    "You can see the list of file system commands by running `hdfs dfs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "uF4zu9qVGIUx"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) [TODO] Let's upload a file to HDFS!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your home directory (`/home/ubuntu/spark_inputs`), you can find the dataset files that we downloaded. Let's upload one of those using HDFS command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "UXTFJvPCGK9s"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "### Please replace the arguments appropriately in the below command\n",
    "# Hint: 1) there are variations: you can use either copyFromLocal or put (or else)\n",
    "#       2) the simplest target directory is '/' (Advanced: Try to create a directory and put the file there)\n",
    "hdfs dfs -put ~/spark_inputs/pagecounts-20160101-000000 hdfs://master:9000/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mqsxmXczfOyC"
   },
   "source": [
    "# 4. Setup YARN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need one more configuration file: `yarn-site.xml`. The only configuration we will add is the address of Resource Manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat ~/yarn-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above, let's copy and paste yarn-site to your hadoop directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "for host in master worker1 worker2\n",
    "do\n",
    "    ssh $host cp ~/yarn-site.xml ~/hadoop/etc/hadoop/yarn-site.xml\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "ATneWfmyfQDy"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "start-yarn.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to `http://<MASTER_IP>:8088`, and enjoy a nice web UI provided by HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8DphHEFufb8y"
   },
   "source": [
    "# 5. Running YARN Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Run one example application in Hadoop distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "D7yqJ1hUffQ7"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "yarn jar ~/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.1.jar \\\n",
    "  -jar ~/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.1.jar \\\n",
    "  -shell_command 'sleep 120; echo hello yarn'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Find the output files in HDFS and see the contents"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "DS2-AWS-hadoop.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
