{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If HDFS is not running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "start-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals of this practice\n",
    "* 1) Learn to write Spark applications \n",
    "* 2) Understand how Spark executes our applications on multiple machines \n",
    "\n",
    "# Wikipedia Pageview statistics\n",
    "\n",
    "https://dumps.wikimedia.org/other/pagecounts-raw/\n",
    "\n",
    "* Hourly Wikipedia statistics are stored in each file\n",
    "* In this practice, we'll use 3 files (= 3 hours of data) that is around 1GB in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# List the 3 files\n",
    "ls -alh /home/ubuntu/spark_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Each 'file' has many 'line's of data\n",
    "* Each 'line' has 4 columns\n",
    "* project | title | number of pageviews | size of the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Print the last 10 'line's of a 'file'\n",
    "tail -n 10 '/home/ubuntu/spark_inputs/pagecounts-20160101-000000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload the files to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Upload the files from the local disk to HDFS\n",
    "hdfs dfs -put \"/home/ubuntu/spark_inputs/\" hdfs://master:9000/wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View the files being uploaded at MASTER_PUBLIC_IP:50070 \n",
    "* (Utilities tab -> 'Browse the file system')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\n",
      "-rw-r--r--   3 ubuntu supergroup  309500921 2018-04-09 06:23 hdfs://master:9000/wiki/pagecounts-20160101-000000\n",
      "-rw-r--r--   3 ubuntu supergroup  369835721 2018-04-09 06:24 hdfs://master:9000/wiki/pagecounts-20160101-010000\n",
      "-rw-r--r--   3 ubuntu supergroup  333752520 2018-04-09 06:24 hdfs://master:9000/wiki/pagecounts-20160101-020000\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# List the uploaded files in the HDFS directory\n",
    "hdfs dfs -ls \"hdfs://master:9000/wiki\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Spark standalone cluster \n",
    "* (instead of YARN for this practice, to easily see the Spark Web UI)\n",
    "* Check the cluster UI => MASTER_PUBLIC_IP:8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org.apache.spark.deploy.master.Master running as process 2335.  Stop it first.\n",
      "[1] 01:43:35 [FAILURE] worker2 Exited with error code 1\n",
      "org.apache.spark.deploy.worker.Worker running as process 1550.  Stop it first.\n",
      "[2] 01:43:35 [FAILURE] worker1 Exited with error code 1\n",
      "org.apache.spark.deploy.worker.Worker running as process 1547.  Stop it first.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "/home/ubuntu/spark_scripts/start_cluster.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy 2 Spark Executors to the standalone cluster\n",
    "* (i.e., initialize Spark Session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "\n",
    "# Required to import pyspark\n",
    "findspark.init('/home/ubuntu/spark')\n",
    "\n",
    "import pyspark\n",
    "\n",
    "# Set executor configurations\n",
    "sparkconf = pyspark.SparkConf().set('spark.executor.memory', '20g')\n",
    "# Deploy Spark executors!!\n",
    "ss = pyspark.sql.SparkSession.builder.appName(\"DS2\").master(\"spark://master:7077\").config(conf=sparkconf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations! 2 Spark executors are now up and running\n",
    "* Check the Spark Web UI => MASTER_PUBLIC_IP:4040\n",
    "* (Check 'Executors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch a Spark MapReduce job\n",
    "* We'll count the sum of pageviews per proejct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Use SparkContext for mapreduce (not SparkSession)\n",
    "sc = ss.sparkContext\n",
    "\n",
    "# Read the data from HDFS\n",
    "lines = sc.textFile(\"hdfs://master:9000/wiki/\")\n",
    "\n",
    "# Split each 'line' into columns\n",
    "columns = lines.map(lambda line: tuple(line.split(\" \")))\n",
    "\n",
    "# Create (project, count) tuples\n",
    "# Be mindful of 'long()'!\n",
    "project_count_tuples = columns.map(lambda column: (column[0], long(column[2])))\n",
    "\n",
    "# For each project, compute the sum of counts\n",
    "project_sum_tuples = project_count_tuples.reduceByKey(lambda left, right: max(left, right)) \n",
    "\n",
    "# Write the output to HDFS\n",
    "# The Spark job starts here! (LAZY EXECUTION)\n",
    "project_sum_tuples.saveAsTextFile(\"hdfs://master:9000/out/\" + str(int(round(time.time()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations! You've just launched Spark 'Tasks' to the 2 executors\n",
    "* Check the Spark Web UI => MASTER_PUBLIC_IP:4040\n",
    "* (Job DAG visualizer / Stages / Task metrics )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the output of the MapReduce job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "hdfs dfs -ls /out/\n",
    "#hdfs dfs -ls /out/...\n",
    "#hdfs dfs -tail /out/.../..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lessons learned: \n",
    "* 1) Spark makes big data easy\n",
    "  * (We ran 6 lines of 'Python code' to analyze big data)\n",
    "  * (Spark took care of the hard problems of distributed execution)\n",
    "* 2) We can handle bigger data by simply adding more EC2 instances\n",
    "  * (Spark will use the additional resources to execute more tasks in parallel)\n",
    "\n",
    "==================================================\n",
    "\n",
    "* Q: I don't even want to write Python code, can I just use SQL on Spark?\n",
    "* A: Yes :)\n",
    "\n",
    "# Launch a SparkSQL job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+\n",
      "|  project|num_of_title|\n",
      "+---------+------------+\n",
      "|       en|     5843326|\n",
      "|commons.m|      982905|\n",
      "|       zh|      763713|\n",
      "|       de|      696433|\n",
      "|       fr|      692983|\n",
      "|       ru|      642028|\n",
      "|       ja|      610854|\n",
      "|       es|      607773|\n",
      "|   www.wd|      443484|\n",
      "|       it|      356246|\n",
      "|       pl|      310897|\n",
      "|       pt|      286648|\n",
      "|     en.d|      219174|\n",
      "|       nl|      181850|\n",
      "|       tr|      178015|\n",
      "|       sv|      138963|\n",
      "|       ko|      127152|\n",
      "|       ar|      122273|\n",
      "|       vi|      108635|\n",
      "|       fa|      100613|\n",
      "+---------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark DataFrame (equivalent of a 'SQL table' in Spark)\n",
    "df = ss.createDataFrame(columns, ['project', 'title', 'count', 'size'])\n",
    "\n",
    "# Create a table view called \"WikipediaTable\"\n",
    "df.createOrReplaceTempView(\"WikipediaTable\")\n",
    "\n",
    "# Run a SQL query over the table\n",
    "# This query orders projects by the number of titles the projects have\n",
    "selected = ss.sql(\"SELECT project, COUNT(title) as num_of_title FROM WikipediaTable GROUP BY project ORDER BY num_of_title DESC\")\n",
    "\n",
    "# Print the results in this console (top 20 results will be shown)\n",
    "selected.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Surprise! You've just launched Spark 'Tasks' to the 2 executors (again!)\n",
    "* How did our SQL query become Spark 'Tasks'?\n",
    "* Check the Spark Web UI again! => MASTER_PUBLIC_IP:4040\n",
    "* Check the SQL tab (SparkSQL Logical operators: Scan, HashAggregate, Project, Exchange, Filter, TakeOrdered, ...)\n",
    "\n",
    "# Lessons learned: \n",
    "* 1) We can use SQL queries (higher-level abstraction) to use Spark\n",
    "  * (Spark automatically translated our query to parallel Spark 'Tasks' to promptly return the result we requested)\n",
    "* 2) Similarly, Spark can automatically translate graph processing, streaming, and machine learning workloads to Spark 'Tasks'\n",
    "  * (some of these topics will be covered in future lectures) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (QUIZ 1) 각 project 당 'count' column에 있는 숫자들의 총합이 20 이상인 (project, sum_of_count)을 구하시오\n",
    "### - 결과값 2개의 column => (project, sum_of_count)\n",
    "# (QUIZ 2) 다음의 table을 'WikipediaTable'과 Join하여, grade가 'A'에 해당하는 project 속하는 title들을 구하시오\n",
    "### - 결과값 1개의 column => (title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|project|grade|\n",
      "+-------+-----+\n",
      "|     en|    C|\n",
      "|     he|    A|\n",
      "|     zh|    B|\n",
      "|     no|    A|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cols = ['project', 'grade']\n",
    "vals = [\n",
    "     ('en', 'C'),\n",
    "     ('he', 'A'),\n",
    "     ('zh', 'B'),    \n",
    "     ('no', 'A')\n",
    "]\n",
    "\n",
    "title_grade = ss.createDataFrame(vals, cols)\n",
    "title_grade.show()\n",
    "title_grade.createOrReplaceTempView(\"TitleGradeTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               title|\n",
      "+--------------------+\n",
      "|                    |\n",
      "|                   $|\n",
      "|%22/fr/Discussion...|\n",
      "|%22/fr/Sp%C3%A9ci...|\n",
      "|                %25s|\n",
      "|  %27Phags-pa-skrift|\n",
      "|%28567%29_Eleutheria|\n",
      "|             %3DOslo|\n",
      "|%60Abdu%27l-Bah%C...|\n",
      "|%60Abdu%27l-Bah%C...|\n",
      "|%C2%AB%C3%86ger%C...|\n",
      "|%C2%ABAdmiral_Hip...|\n",
      "|%C2%ABAdmiral_Sch...|\n",
      "|%C2%ABAdmiral_Sch...|\n",
      "| %C2%ABAltmark%C2%BB|\n",
      "|%C2%ABAnglo_Norse...|\n",
      "|%C2%ABBarbarossa%...|\n",
      "|%C2%ABCutty_Sark%...|\n",
      "|%C2%ABDeutschland...|\n",
      "|%C2%ABEidsvold_18...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selected = ss.sql(\"SELECT title FROM WikipediaTable NATURAL JOIN TitleGradeTable WHERE grade='A'\")\n",
    "selected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = ss.sql(\"SELECT project, SUM(count) as sum_of_count FROM WikipediaTable GROUP BY project HAVING sum_of_count >20\")\n",
    "selected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = ss.sql(\"SELECT title FROM WikipediaTable NATURAL JOIN TitleGradeTable WHERE grade='A'\")\n",
    "selected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop Spark executors and the Spark cluster using the following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "/home/ubuntu/spark_scripts/stop_cluster.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
