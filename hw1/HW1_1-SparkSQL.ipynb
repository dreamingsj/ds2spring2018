{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [[HW 1_1 SparkSQL 숙제]]\n",
    "\n",
    "# HDFS 실행 (MASTER_PUBLIC_IP:50070로 실행 확인)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "start-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia data가 HDFS에 올라와 있는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# List the uploaded files in the HDFS directory\n",
    "hdfs dfs -ls \"hdfs://master:9000/wiki\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 만약 올라와 있지 않다면, 데이터 업로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Upload the files from the local disk to HDFS\n",
    "hdfs dfs -put \"/home/ubuntu/spark_inputs/\" hdfs://master:9000/wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (혹시 실수로 데이터 업로드 두번 하셨다면, 다음 커맨드를 사용해서 중복 데이터를 제거해주세요)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# hdfs dfs -rm -R \"hdfs://master:9000/wiki/spark_inputs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 스파크 분산환경 셋팅 (MASTER_PUBLIC_IP:8080으로 실행 확인)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "/home/ubuntu/spark_scripts/start_cluster.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 스파크 실행 (MASTER_PUBLIC_IP:4040으로 실행 확인)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "\n",
    "# Required to import pyspark\n",
    "findspark.init('/home/ubuntu/spark')\n",
    "\n",
    "import pyspark\n",
    "\n",
    "# Set executor configurations\n",
    "sparkconf = pyspark.SparkConf().set('spark.executor.memory', '20g')\n",
    "\n",
    "# Deploy Spark executors!!\n",
    "ss = pyspark.sql.SparkSession.builder.appName(\"DS2\").master(\"spark://master:7077\").config(conf=sparkconf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 스파크 Dataframe 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SparkContext for mapreduce (not SparkSession)\n",
    "sc = ss.sparkContext\n",
    "\n",
    "# Read the data from HDFS\n",
    "lines = sc.textFile(\"hdfs://master:9000/wiki/\")\n",
    "\n",
    "# Split each 'line' into columns\n",
    "columns = lines.map(lambda line: tuple(line.split(\" \")))\n",
    "\n",
    "# Create a Spark DataFrame (equivalent of a 'SQL table' in Spark)\n",
    "df = ss.createDataFrame(columns, ['project', 'title', 'count', 'size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 숙제 SQL query 실행 및 결과물 콘솔에 프린트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.createOrReplaceTempView(\"....\")\n",
    "# selected = ss.sql(\"....\")\n",
    "# selected.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
